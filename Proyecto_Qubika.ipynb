{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sebas-gifPaz777/Qubika_reto/blob/main/Proyecto_Qubika.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NmnHFaDVZRV"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import re\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "!pip install langchain_openai langchain_community\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.document_loaders import TextLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BeautifulSoup para web scrapping"
      ],
      "metadata": {
        "id": "7czRuaFFZHva"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pW-8xDN6VZRY"
      },
      "outputs": [],
      "source": [
        "url = \"https://www.bbc.com/mundo/topics/c06gq9v4xp3t?page=1\"\n",
        "pages = 20\n",
        "def scrape_bbc_news(url,pages):\n",
        "\n",
        "    # Lista para almacenar los resultados\n",
        "    news_data = []\n",
        "    for i in range(pages) :\n",
        "        parts = re.split(\"page\",url)\n",
        "        url = str(parts[0])+'page='+str(i+1)\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        # Extraer noticias\n",
        "        articles = soup.find_all(\"li\", class_=\"bbc-t44f9r\")\n",
        "\n",
        "\n",
        "        for article in articles:\n",
        "            link = article.find(\"a\", href=True)[\"href\"] if article.find(\"a\", href=True) else \"No link\"\n",
        "            if(link != \"No link\"):\n",
        "                response = requests.get(link)\n",
        "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "                title = soup.find(\"h1\", class_=\"bbc-14gqcmb e1p3vdyi0\").text if soup.find(\"h1\", class_=\"bbc-14gqcmb e1p3vdyi0\") else \"No title\"\n",
        "                content =\" \".join([p.get_text() for p in soup.find_all('p', class_=\"bbc-hhl7in e17g058b0\")])\n",
        "                content = re.split(\"Haz clic aquí para leer más historias\",content)[0].strip()\n",
        "                news_data.append({\n",
        "                    \"title\": title,\n",
        "                    \"content\": content\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(news_data)\n",
        "\n",
        "# Ejecutar scraping\n",
        "df_news = scrape_bbc_news(url,pages)\n",
        "print(df_news.__len__())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Limpieza de datos"
      ],
      "metadata": {
        "id": "Zt3Y0LoDbyqK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzdc9xN_VZRa"
      },
      "outputs": [],
      "source": [
        "df_news_unique = df_news.drop_duplicates(subset=[\"title\"])\n",
        "df_news_unique = df_news_unique.drop_duplicates(subset=[\"content\"])\n",
        "\n",
        "print(f\"Noticias originales: {len(df_news)}, Noticias sin duplicados: {len(df_news_unique)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjfmHj53VZRa"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convertir a minúsculas\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Eliminar espacios múltiples\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Eliminar caracteres especiales\n",
        "    return text.strip()\n",
        "\n",
        "# Aplicar preprocesamiento\n",
        "df_news_unique['title'] = df_news_unique['title'].apply(preprocess_text)\n",
        "df_news_unique['content'] = df_news_unique['content'].apply(preprocess_text)\n",
        "\n",
        "# Mostrar algunos ejemplos\n",
        "print(df_news_unique.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_news_unique.to_csv(\"news.csv\", index=False)"
      ],
      "metadata": {
        "id": "Kff8PJZPJUOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Se añaden los datos a la base de datos vectorial"
      ],
      "metadata": {
        "id": "VepLkbLJcEWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_news_unique = pd.read_csv(\"news.csv\")"
      ],
      "metadata": {
        "id": "K8_6LuGHG3mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document"
      ],
      "metadata": {
        "id": "3sJl4DM5KfoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VrhKs4AVZRc"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL97HJfeWoMO"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1T1HRhJ8VZRd"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from chromadb.utils import embedding_functions\n",
        "from chromadb import Client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = \"chromadb_data\"\n",
        "client = Client(Settings(persist_directory=persist_directory))\n",
        "\n",
        "# Crear colección\n",
        "collection_name = \"news_collection9\"\n",
        "collection = client.get_or_create_collection(name=collection_name)\n",
        "\n",
        "# Modelo de Hugging Face para embeddings\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')  # Cambiar por otro modelo si prefieres\n",
        "\n",
        "# Lista de documentos combinados\n",
        "list_documents = []\n",
        "for index, row in df_news_unique.iterrows():\n",
        "    list_documents.append(str(row['title']) + \" \" + str(row['content']))\n",
        "\n",
        "# Crear objetos Document\n",
        "documents = [Document(page_content=text) for text in list_documents]\n",
        "\n",
        "print(documents[3].page_content)"
      ],
      "metadata": {
        "id": "Sll2DcFjieBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir en chunks\n",
        "def split_text(documents):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=400,\n",
        "        chunk_overlap=100,\n",
        "        length_function=len,\n",
        "        add_start_index=True,\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    return chunks\n",
        "\n",
        "# Crear chunks para todos los documentos\n",
        "chunks_list = split_text(documents)\n",
        "\n",
        "# Generar embeddings para los chunks\n",
        "texts = [chunk.page_content for chunk in chunks_list]\n",
        "metadatas = [{\"start_index\": chunk.metadata[\"start_index\"]} for chunk in chunks_list]\n",
        "embeddings = model.encode(texts, show_progress_bar=True)\n",
        "\n",
        "# Agregar los chunks a ChromaDB\n",
        "ids = [f\"chunk_{i}\" for i in range(len(texts))]\n",
        "\n",
        "collection.add(\n",
        "    ids=ids,\n",
        "    documents=texts,\n",
        "    metadatas=metadatas,\n",
        "    embeddings=embeddings,\n",
        ")\n",
        "\n",
        "# Confirmar almacenamiento\n",
        "print(f\"Se almacenaron {collection.count()} chunks en la colección.\")\n",
        "\n",
        "# Realizar una consulta\n",
        "query = \"trump y elon musk\"\n",
        "query_embedding = model.encode([query])[0]\n",
        "\n",
        "results = collection.query(query_embeddings=[query_embedding], n_results=2)\n",
        "print(\"Resultados de la búsqueda:\")\n",
        "for doc, metadata in zip(results[\"documents\"], results[\"metadatas\"],results[\"ids\"]):\n",
        "    print(f\"Documento: {doc}\")\n",
        "    print(f\"Metadata: {metadata}\")\n"
      ],
      "metadata": {
        "id": "rI5ZNxKnHbzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "PSqnwJ0KDVuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_docs = collection.get(include=[\"documents\", \"metadatas\",\"embeddings\"])\n",
        "all_docs[\"embeddings\"] = [embedding.tolist() for embedding in all_docs[\"embeddings\"]]\n",
        "\n",
        "# Guardar en un archivo JSON\n",
        "with open(\"collection_backup.json\", \"w\") as f:\n",
        "    json.dump(all_docs, f)"
      ],
      "metadata": {
        "id": "WSJCTuin8nP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG (Retrival Augmented Generation)"
      ],
      "metadata": {
        "id": "W90hECCacO2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "Q9j8fsob3sDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_Cb4vLDVZRg"
      },
      "outputs": [],
      "source": [
        "# Cargar el modelo generativo\n",
        "genai.configure(api_key=\"AIzaSyCFma-VNWVHgNPKHrFnfuyHKc8q1t8klT0\")\n",
        "\n",
        "modelGemini = genai.GenerativeModel(\n",
        "    model_name='gemini-1.5-pro',\n",
        "    tools='code_execution')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GzNBbLFVZRg"
      },
      "outputs": [],
      "source": [
        "def retrieve_documents(query, top_k=2):\n",
        "    query_embedding = model.encode([query])[0]\n",
        "    results = collection.query(query_embeddings=[query_embedding], n_results=top_k)\n",
        "    retrieved_docs = [doc for doc, metadata in zip(results[\"documents\"], results[\"metadatas\"])]\n",
        "    print(retrieved_docs)\n",
        "    return retrieved_docs\n",
        "\n",
        "def generate_answer(query, top_k=3):\n",
        "    retrieved_docs = retrieve_documents(query, top_k=top_k)\n",
        "    context = \"\\n\".join(str(retrieved_docs))\n",
        "    prompt = f\"A continuación tenemos el siguiente contexto separado con saltos de linea, las partes que esten relacionados con la consulta usalos: {context}\\n ---------------- \\n Responde la consulta teniendo en cuenta las partes de contexto utiles y hazlo en español: {query}\\n --------------- \\nRespuesta:\"\n",
        "    response = modelGemini.generate_content((prompt))\n",
        "    #response = generator(prompt, max_length=max_length, num_return_sequences=1)\n",
        "    answer = response.text\n",
        "    return answer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluación"
      ],
      "metadata": {
        "id": "EmvhhI_bciTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepeval"
      ],
      "metadata": {
        "id": "ZkhMf5SIZ6FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PReFvbeXVZRg"
      },
      "outputs": [],
      "source": [
        "from deepeval import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcumHcYuVZRg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Función para evaluar la calidad de las respuestas\n",
        "def evaluate_responses(queries, ground_truths, top_k=3):\n",
        "    generated_responses = [generate_answer(query, top_k) for query in queries]\n",
        "    metrics = evaluate(generated_responses, ground_truths)\n",
        "\n",
        "    print(\"Resultados de Evaluación:\")\n",
        "    print(\"Exactitud:\", metrics['accuracy'])\n",
        "    print(\"BLEU Score:\", metrics['bleu'])\n",
        "    print(\"ROUGE-L:\", metrics['rouge'])\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUlPRz90VZRh"
      },
      "outputs": [],
      "source": [
        "queries = [\n",
        "    \"Hablame sobre el megapuerto chancay en perú\"\n",
        "]\n",
        "\n",
        "# Respuestas esperadas (ground truths)\n",
        "ground_truths = [\n",
        "    \"El presidente electo de Estados Unidos, Donald Trump, anunció este martes que encargó a Elon Musk, propietario de Tesla y actualmente el hombre más rico del mundo, que lidere junto al ex candidato presidencial republicano Vivek Ramaswamy.\"\n",
        "]\n",
        "\n",
        "#RAG\n",
        "generated_responses = [generate_answer(query) for query in queries]\n",
        "print(generated_responses)\n",
        "# Evaluar\n",
        "metrics = evaluate_responses(queries, ground_truths)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Comparación de noticias (BBC y El Tiempo)"
      ],
      "metadata": {
        "id": "mTsvxDTjcmlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def web_scraping_bbc_new(url):\n",
        "  new_data = \"\"\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "  title = soup.find(\"h1\", class_=\"bbc-14gqcmb e1p3vdyi0\").text if soup.find(\"h1\", class_=\"bbc-14gqcmb e1p3vdyi0\") else \"No title\"\n",
        "  content =\" \".join([p.get_text() for p in soup.find_all('p', class_=\"bbc-hhl7in e17g058b0\")])\n",
        "  content = re.split(\"Haz clic aquí para leer más historias\",content)[0].strip()\n",
        "  new_data+=f\"{title} \\n{content}\"\n",
        "  print(new_data)\n",
        "  return new_data"
      ],
      "metadata": {
        "id": "hdjM4wDTpfY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def web_scraping_el_tiempo_new(url):\n",
        "  new_data = \"\"\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "  div = soup.find(\"div\", class_=\"c-detail-content\")\n",
        "  title = soup.find(\"h1\", class_=\"c-detail__title\").text if soup.find(\"h1\", class_=\"c-detail__title\") else \"No title\"\n",
        "\n",
        "  paragraphs = soup.find_all(\"div\", class_=\"paragraph\")\n",
        "  content = \"\"\n",
        "  for paragraph in paragraphs:\n",
        "      for child in paragraph.children:\n",
        "          if child.name == \"b\":\n",
        "              # Si es una etiqueta <b>\n",
        "              content +=f\"{child.get_text(strip=True)} \"\n",
        "          elif child.name is None:\n",
        "              # Si es un texto sin etiqueta\n",
        "              print(\"Entra no bold\")\n",
        "              content+=child.strip()+\" \"\n",
        "\n",
        "  new_data+=f\"{title} \\n{content}\"\n",
        "  print(new_data)\n",
        "  return new_data"
      ],
      "metadata": {
        "id": "jbzjviHnwGJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Cargar el modelo de resumen\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Función para dividir el texto en fragmentos manejables\n",
        "def split_text(text, max_length=1024):\n",
        "    sentences = text.split(\". \")  # Dividir por oraciones\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) <= max_length:\n",
        "            current_chunk += sentence + \". \"\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \". \"\n",
        "\n",
        "    # Agregar el último fragmento\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Fragmentar el texto\n",
        "news_1 = split_text(web_scraping_bbc_new(\"https://www.bbc.com/mundo/articles/cnvj2z7evp8o\"))\n",
        "news_2 = split_text(web_scraping_el_tiempo_new(\"https://www.eltiempo.com/mundo/eeuu-y-canada/donald-trump-nombra-a-elon-musk-como-director-del-departamento-de-eficiencia-gubernamental-de-estados-unidos-3399054\"))\n",
        "\n",
        "\n",
        "# Generar resúmenes para cada fragmento\n",
        "\n",
        "def summarize_text(text_chunks):\n",
        "  summaries = []\n",
        "  for chunk in text_chunks:\n",
        "      summary = summarizer(chunk, max_length=100, min_length=50, do_sample=False)[0]['summary_text']\n",
        "      summaries.append(summary)\n",
        "  final_summary = \" \".join(summaries)\n",
        "  return final_summary\n",
        "\n",
        "news_1 = summarize_text(news_1)\n",
        "news_2 = summarize_text(news_2)\n",
        "\n",
        "print(\"Resumen completo 1:\")\n",
        "print(news_1)\n",
        "\n",
        "print(\"Resumen completo 2:\")\n",
        "print(news_2)"
      ],
      "metadata": {
        "id": "-p0Wt8hGzeId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = generate_answer(f\"Compara estas dos noticias, noticia 1:{news_1} \\n noticia 2:{news_2}\")"
      ],
      "metadata": {
        "id": "tnCCQy1OJeTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Narración de la respuesta"
      ],
      "metadata": {
        "id": "zR7wVzd8cu9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install TTS"
      ],
      "metadata": {
        "id": "_Tu2Xa0tOKGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from TTS.api import TTS\n",
        "\n",
        "# Cargar el modelo preentrenado\n",
        "tts = TTS(model_name=\"tts_models/es/mai/tacotron2-DDC\", progress_bar=True)\n",
        "\n",
        "# Texto a convertir a voz\n",
        "text = \"Este es un ejemplo de narración en español usando Coqui TTS.\"\n",
        "\n",
        "# Guardar como archivo de audio\n",
        "tts.tts_to_file(text=answer, file_path=\"narracion_coqui.wav\")\n",
        "print(\"Archivo guardado como narracion_coqui.wav\")"
      ],
      "metadata": {
        "id": "u5c4IlZHOMBY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}